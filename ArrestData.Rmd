---
title: "MATH4939 Project Arrest Dataset"
author: "Team 3"
date: "09/04/2020"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(spida2)
library(ggplot2)
library(car)
library(carData)
library(pROC)
library(mosaic)
library(gridExtra)
```

# Introduction

“Arrests” dataset is based on the police treatment of individuals arrested in Toronto for possession of small amounts of marijuana from 1997 to 2002. The dataset is just a part of a large data set mentioned in a series of articles in the Toronto star.  The dataset contains 5226 observations with 8 variables as below. 

released: whether or not the person who is arrested is released with a summon (Yes or No) 
xcolour: The arrested persons race (Black or White) 
year: 1997 - 2002
age: The age of the arrested person in years 
sex: Gender of the arrested person (Male or Female) 
employed: Is the arrested person employed (Yes or No) 
citizen: Is the person a citizen of toronto (Yes or No) 
checks: Number obtained from the police databases (of previous arrests, previous conviction, parole status, etc.) the arrested persons name appeared upon labeled from 1 to 6
  
According to the dataset, the variable “released” is the independent variable y, and the rest are dependent variables. In this project, we will build two models using logistic regression method, compare the two models in different ways, and find out the factors which can influence the independent variable “released” significantly in order to explore the patterns of discrimination in the dataset.


```{r car, echo=FALSE}
summary(Arrests)
```

# Motivation
We will build two models for the dataset. The first model is a simple model, which is : $$logit(\pi) = \beta_0 +\beta_1x_1 +\beta_2x_2 + \beta_3x_3+ \beta_4x_4 +\beta_5x_5 +\beta_6x_6 +\beta_7x_7$$. Another model is more complicated, which contains the interaction terms and quadratic terms. We are going to compare these two models in different ways and use the better one to find out the potential patterns of discrimination.

# Model Selection

Based on the given data set we build an interaction model which will be our base. 

```{r, echo=FALSE}
model1<- glm(released ~ (colour + year + age + sex + citizen + employed + checks)^2,data = Arrests, family = "binomial")
model1

```
After evaluating the model using wald test we see that there are some p-values in the predictors that may cause problems in further analysis. 
```{r, echo=FALSE}
wald(model1)
```
Thus we run stepwise regression to obtain the selected model which we will use to compare with the additative model. 
```{r, echo=FALSE}
#step(model1, direction = c("both"))
```

Based on the stepwise regression we end up with our selected model which is \begin{multline*} $$logit(\pi(released)) = \beta_0 + \beta_1colour + \beta_2year + \beta_3age +\beta_4sex +\beta_5employed +\beta_6citizen+\beta_7checks+ \beta_8colour*year \\ + \beta_9colour*age + \beta_{10}year*sex + \beta_{11}year*employed +\beta_{12}year*checks +\\\beta_{13}age*employed + \beta_{14}sex*employed +\beta_{15} citizen*checks$$
\end{multline*}

```{r, echo=FALSE}
model2 <- glm(released~ colour+ year + age + sex+ citizen+ employed+ checks + colour*year + colour*age + year*sex + year*employed + year*checks + age*employed  + sex*employed + citizen*checks, data = Arrests, family = "binomial")
model2
```

# Model Comparison

We compared the simple additive model $$logit(\pi(released))=\beta_0 +\beta_1colour +\beta_2year + \beta_3age+ \beta_4sex +\beta_5employed +\beta_6citizen +\beta_7checks$$ with the model we get after selection in different ways, which is  \begin{multline*}
$$logit(\pi(released)) = \beta_0 + \beta_1colour + \beta_2year + \beta_3age +\beta_4sex +\beta_5employed +\beta_6citizen+\beta_7checks+ \\\beta_8colour*year + \beta_9colour*age + \\\beta_{10}year*sex + \beta_{11}year*employed +\beta_{12}year*checks + \beta_{13}age*employed + \beta_{14}sex*employed +\beta_{15} citizen*checks$$ \end{multline*}




## 1.Comparison in Overall Model Significance

We did wald test for the two models to compare their model significance.

### For the Additive Model
```{r, echo=FALSE}
fit <- glm(released~ colour+ year + age + sex+ citizen+ employed+     checks,family="binomial",data=Arrests)
wald(fit)
```

### For the Selected Model 

```{r, echo=FALSE}
wald(model2)
```

Comparing these two outputs, we found that the overall p-values in both models are small, which means these two models are both significant. For the model we selected, the p-values for the variable year, age and sex are smaller that the p-values of additive model, which means these factors become more significant in the model we selected. What’s more, in the model we selected, p-values are very small for most of the interaction terms. Thus, we can conclude that the significance gets improved using the model we selected.

## 2. Comparision in ANOVA

```{r, echo=FALSE}
anova(fit,model2, test = "LRT")
```

According to the ANOVA table, the residual deviance of model 2 is smaller than that of model 1, which means the model we selected is better than the additive model.

## 3.Comparison in AIC
```{r,echo=FALSE}
AIC(model2,fit)
```

According to the output of AIC, the value of AIC in the model we selected is much smaller than the additive model. Thus, in this aspect, the model we selected is better than the additive model.

# Model Evaluation
## 1.  Comparison in Multicollinearity (VIF)
```{r,echo=FALSE}
car::vif(model2)
```

In the selected model, the VIF score for the predictor variables have some predictors that are higher than 10, we can say that there is some form of multicollinearity that exists in this model. We see that since our model contains alot of interaction terms we concluded that VIF scores are high because of the amount of interaction terms we have in the model and waved off the multicollinearity theory for our selected model.

## 2. Prediction Performance Evaluation
```{r,echo=FALSE}
set.seed(123)
id <- sample(nrow(Arrests), 0.8*nrow(Arrests))
train <- Arrests[id,]
test <- Arrests[-id,]

model<-glm(released~ colour+ year + age + sex+ citizen+ employed+ checks + colour*year + colour*age + year*sex + year*employed + year*checks + age*employed  + sex*employed + citizen*checks, data = train, family = "binomial")

pred <- predict(model, train, type = 'response')
predclass <- ifelse(pred > 0.5, "Yes", "No")
pred2 <- predict(model, test, type = "response")
predclass2 <- ifelse(pred2 > 0.5, "Yes", "No")

mean(predclass == train$released)
mean(predclass2 == test$released)

pred3<-predict(model2, Arrests, type = "response")
predclass3 <- ifelse(pred3 > 0.5, "Yes", "No")
mean(predclass3 == Arrests$released)


modelROC<- roc(test$released, pred2)
plot(modelROC, print.auc = TRUE, auc.polygon = TRUE, grid = c(0.1,0.2)
     , grid.col = c("green", "red"), max.auc.ploygon = TRUE, auc.polygon.col = "skyblue", print.thres = TRUE)

```
Based on the best logistic model selected, using a random split 80% as training data and left 20% as testing data, the logistic model has an about 83.4% accuracy in predicting released status in training set and about 83.1% accuracy in predicting released status in testing set, and about 83.3% accuracy in predicting released status using whole data. This shows the best logistic model is consistent in prediction performance, the inferences based on the best logistic model is reliable which means given new data, the model would also performs well compare with a random guess with 50% accuracy only. The AUC on testing set is 0.75 which is high, so the model prediction performance is well


# Model Analysis 

## Interpretation of  betai  in Selected Prediction Model

```{r, echo=FALSE}
coef(model2)
```

Since the model we selected is : \begin{multline*}
$$logit(\pi(released)) = \beta_0 + \beta_1colour + \beta_2year + \beta_3age +\beta_4sex +\beta_5employed +\beta_6citizen+\beta_7checks+\\ \beta_8colour*year + \beta_9colour*age + \\\beta_{10}year*sex + \beta_{11}year*employed +\beta_{12}year*checks +\beta_{13}age*employed + \beta_{14}sex*employed +\beta_{15} citizen*checks$$
\end{multline*}
 According to the output, $\beta_1$ = 366.201678, $\beta_2$= 0.082462, $\beta_3$ = 0.044486,  $\beta_4$= -571.63776988 ,  $\beta_5$= 0.92474947, $\beta_6$=  313.81186106 , $\beta_7$= 114.39688397, $\beta_8$= -0.18253853, $\beta_9$=  -0.03433990, $\beta_{10}$= 0.28606233, $\beta_{11}$= -0.15598455, $\beta_{12}$= -0.05733875, $\beta_{13}$= -0.02597960, $\beta_{14}$= -0.59402355, $\beta_{15}$= -0.15067117. To interpret $\beta_i$, we can take partial derivative of $logit(\pi(released))$ with respect to independent variables.

For example, if we take partial derivative with respect to ‘colour’, we will get :

$$\frac{dlogit(\pi (released))}{dcolour} = \beta_1 + \beta_8*year + \beta_9*age$$

Thus beta1 is the partial derivative of log odds of dependent variable ‘released’ with respect to ‘colour’ when year=age=0. 

Similarly, if we take partial derivative with respect to ‘year’, we will 
get :  

$$\frac{dlogit(\pi (released))}{dyear} = \beta_2 + \beta_8*colour + \beta_{10}*sex + \beta_{11}*employed + \beta_{12}*checks$$

Thus $\beta_2$ is the partial derivative of log odds of dependent variable ‘released’ with respect to ‘year’ when colour=sex=employed=checks=0. For other $\beta_i$ , we also take partial derivative with respect to each independent variable, since the interpretation methods are similar, we do not enter into details here.


## Wald Test for Some Predictor Variables in Selected Model

### 1. Wald Test of Terms Involving ‘colour’
```{r,echo=FALSE}
wald(model2, "colour")
```
We did wald test for all the terms involving the ‘colour’ variable. We can see from the output that p-values for all terms which involve ‘colour’ are small, which means they are all significant. Thus we can conclude that the variable ‘colour’ influences the response variable ‘released’ significantly. It seems that there exists racial discrimination as for this data set. 

### 2. Wald Test of Terms Involving ‘year’

```{r,echo=FALSE}
wald(model2, "year")
```

We are interested in the variable ‘year’ since the p-value is very large in the additive model which is equal to 0.88184. But in the selected model, p-value is smaller, which is equal to 0.52641. Thus the significance of variable ‘year’ is enhanced after we add interaction terms between ‘year’ and other variables. We can see from the output that all the interaction terms including ‘year’ are significant with small p-values. In this way, we can conclude that although ‘year’ individually does not have a strong relationship with ‘released’, the terms where ‘year’ interacts with other variables are important when we do the analysis. Therefore, we can analyze the relationship between predictor variables such as  colour and the response variable ‘released’ in different years.

### 3.Wald Test of Terms Involving ‘age’
```{r,echo=FALSE}
wald(model2, "age")
```

We did wald test for all the terms involving the age variable. We can see from the output that p-values for all terms which involve age are small, which means they are all significant. The interesting thing is the factor ‘age’ is not significant in additive model with p-value is equal to 0.62932. Thus probably the interaction between age and other variables enhances the significance of age. So we can focus on the interaction of age and other predictor factors

### 4. Wald Test of All Interaction Terms

```{r,echo=FALSE}
wald(model2, ":")
```
According to the output of wald test for all interaction terms, although for some terms the p-values are not that small, the overall p-value for interaction terms are really small. Thus to involve interaction terms in the model is still meaningful, we can choose some of the significant interaction terms to explore their relationships with response variable.

## Added Variable plot

```{r,echo=FALSE}
library(car)
avPlots(glm(released ~ colour + year + age + sex + citizen + employed + checks + colour*year + colour*age + year*sex + year*employed + year*checks + age*employed + sex*employed + citizen*checks, data = Arrests, family = "binomial"))

```
The Added Variable plot helps us evaluate the residuals and coefficients of the predictors while holding other variables constant. In other words it helps us answer the question if the effect of a particular predictor on the dependant variable while holding other predictors constant

These plots show the value of each predictor after other predictors are accounted for on the X-axis and value of the dependent variable after other predictors are accounted for on the Y-axis

We can see which predictor has the strongest influence on dependant variable (released), after accounting for all other predictors, this is done by evaluating the slope of each chart in the grid 

After analyzing and calculating the slopes of each grid. I come to a conclusion claiming that the interaction term based on colour have the strongest influence out of all the predictor, especially colour*year. 

## Data Visualization and Analysis
### Xq plot
```{r,echo=FALSE}
xqplot(Arrests)
```
From The Xq plots, we can get basic information about the data set. For example, there are much more males than females. In addition, the number of white people is more than twice the number of black people. The number of people who are employed is also far more than those who are not employed

### Visualization plot
```{r , echo=FALSE}
fit1<-ggplot(Arrests, aes(x=colour,y=released,colour=released,fill=released)) + 
  geom_bar(aes(y=..count..))+
  geom_text(aes(label=released),vjust=1.5)+
  scale_color_manual(values = c("paleturquoise1", "sienna1"))+
  scale_fill_manual(values = c("lightsteelblue2", "tan1"))+
  labs(x="colour", y = "released",title = "released ~ colour")+
  theme(legend.position="top")+
  theme_set(theme_bw())+
  theme(legend.title = element_text(color = "lightblue",size = 14, face = "bold"))
```
In the color and released histogram, it is clear that color is a very important factor. The reason is that whites are released in significantly higher Numbers than blacks. We have to suspect that there is racial discrimination. Similarly, whites arrest fewer people than blacks. And more whites own marijuana than blacks do.
```{r , echo=FALSE}
fit2<-ggplot(Arrests, aes(x=year,y=released,colour=released,fill=released))+ 
  geom_histogram(aes(y=..density..),alpha=0.5, bins=30, position="identity")+
  scale_color_manual(values = c("paleturquoise1", "sienna1"))+
  scale_fill_manual(values = c("lightsteelblue2", "tan1"))+
  labs(x="year", y = "released",title = "released~ year")+
  theme(legend.position="top")+
  theme_set(theme_bw())+
  theme(legend.title = element_text(color = "lightblue",size = 14, face = "bold"))
```

As can be seen from the figure, the number of arrests and releases increased from 1997 to 2000, reaching a peak in 2000 and the second highest in 2001. My wild guess is that it may have something to do with the 2000 U.S. presidential election and the events of 9/11. Some Canadians were unhappy with the Bush presidency and began to possess marijuana, as well as the damage to the global economy and fears about security caused by 9/11. Second, it is clear that most of those arrested each year are released. In 2002, marijuana possession was brought under control.
```{r , echo=FALSE}
fit3<-ggplot(Arrests, aes(x=sex,y=released,colour=released,fill=released)) + 
  geom_bar(aes(y=..count..))+
  geom_text(aes(label=released),vjust=1.5)+
  scale_color_manual(values = c("paleturquoise1", "sienna1"))+
  scale_fill_manual(values = c("lightsteelblue2", "tan1"))+
  labs(x="sex", y = "released",title = "released ~ sex")+
  theme(legend.position="top")+
  theme_set(theme_bw())+
  theme(legend.title = element_text(color = "lightblue",size = 14, face = "bold"))
```
From the figure, it is obvious that the arrest rate of men is much higher than that of women. This may have to do with the fact that men are more stressed than women. In general, men's jobs are more challenging than women's, so they may need marijuana to relax.
```{r , echo=FALSE}
fit4<-ggplot(Arrests, aes(x=employed,y=released,colour=released,fill=released)) + 
  geom_bar(aes(y=..count..))+
  scale_color_manual(values = c("paleturquoise1", "sienna1"))+
  scale_fill_manual(values = c("lightsteelblue2", "tan1"))+
  labs(x="employed", y = "released",title = "released ~ employed")+
  theme(legend.position="top")+
  theme_set(theme_bw())+
  theme(legend.title = element_text(color = "lightblue",size = 14, face = "bold"))
```
As can be seen from the histogram, people with jobs are more likely to be released. Because people who are employed are more likely to give up marijuana than people who aren't.


```{r , echo=FALSE}
grid.arrange(fit1, fit2,fit3,fit4, ncol = 2,nrow=2)
```
With ggplot, we can only make a basic observation and prediction of the data. More detailed analysis, we also need to build a model, more complex analysis. And each variable may affect the other. Could, for example, employed white Toronto residents be the most likely to be released?

### XYplot

```{r, echol=FALSE}
myplot = xyplot(jitter( as.integer(released)-1) ~ age, data=Arrests, alpha=0.5, pch=19, cex=1, ylab="Is Released")
fit.outcome = makeFun(model2)
myplot

```
Based on the best logistic model, now we use xyplot to investigate two main interested questions?
1). whether employed is helpful for improving probability of releaseing after arrested?
2). whether being a citizen is helpful for improving probability of releaseing after arrested?
Here, we take the most recent year 2002, and for a Black male who is firstly arrested(checks = 0) to investigate the questions using xyplots firstly and then we do the same for a white male with same conditions, we do it seperately for white and black people as there seems some differences between white and black in probability of released as discrination of race existed:

```{r,echo=FALSE}
myplot2 = plotFun(fit.outcome(year = 2002, age = x, colour ="Black", sex = "Male", 
                   employed = "No", citizen = "No", checks = 1) ~ x, lwd=3, 
                  col = "yellow", plot=myplot, add=TRUE)
plotFun(fit.outcome(year = 2002, age = x, colour ="Black", sex = "Male", 
                   employed = "Yes", citizen = "No", checks = 1) ~ x, 
         lwd=3, plot=myplot2, col = "red", add=TRUE) 

```

From the above xyplot, given the people is no citizen, it can be founded that for youngers(age < 35), being employed has a much higher released probability than those not being employed as the redline(employed) is much higher the yellowline(not employed) in the xyplot, and they are close for old people as for old people, most of them are already not employed(retired).

```{r,echo=FALSE}
myplot2 = plotFun(fit.outcome(year = 2002, age = x, colour ="Black", sex = "Male", 
                   employed = "No", citizen = "No", checks = 1) ~ x, lwd=3, 
                  col = "yellow", plot=myplot, add=TRUE)
plotFun(fit.outcome(year = 2002, age = x, colour ="Black", sex = "Male", 
                   employed = "No", citizen = "Yes", checks = 1) ~ x, 
         lwd=3, plot=myplot2, col = "red", add=TRUE)

```

From the above xyplot, given the people is not employed, it can be founded that being citizen has a much higher released probability than those not being citizen as the redline(citizen) is much higher the yellowline(not citizen) in the xyplot.

```{r,echo=FALSE}
myplot2 = plotFun(fit.outcome(year = 2002, age = x, colour ="White", sex = "Male", 
                   employed = "No", citizen = "No", checks = 1) ~ x, lwd=3, 
                  col = "yellow", plot=myplot, add=TRUE)
plotFun(fit.outcome(year = 2002, age = x, colour ="White", sex = "Male", 
                   employed = "Yes", citizen = "No", checks = 1) ~ x, 
         lwd=3, plot=myplot2, col = "red", add=TRUE)

```

From the above xyplot, given the people is no citizen, it can be founded that for youngers, being employed has a much higher released probability than those not being employed as the redline(employed) is much higher the yellowline(not employed) in the xyplot, but for olders, being employed has a little lower released probability than those not being employed as the redline(employed) is little lower than the yellowline(not employed).

```{r,echo=FALSE}
myplot2 = plotFun(fit.outcome(year = 2002, age = x, colour ="White", sex = "Male", 
                   employed = "No", citizen = "No", checks = 1) ~ x, lwd=3, 
                  col = "yellow", plot=myplot, add=TRUE)
plotFun(fit.outcome(year = 2002, age = x, colour ="White", sex = "Male", 
                   employed = "No", citizen = "Yes", checks = 1) ~ x, 
         lwd=3, plot=myplot2, col = "red", add=TRUE)

```
From the above xyplot, given the people is not employed, it can be founded that being citizen has a much higher released probability than those not being citizen as the redline(citizen) is much higher the yellowline(not citizen) in the xyplot.
So that, overall, we can conclude that being employed and being citizen could help improving probability of releaseing after arrested a lot for both white and black young people.

## Frequency bar charts
### The Relationship between ‘colour’ and ‘released’ as year changes
```{r,echo=FALSE}
tab__(Arrests, ~  colour +year + released) %>%
    barchart(ylab = 'count',
             ylim = c(0,1000),
              #  box.width = rep(.9^c(2,1,1,1,1,1), each = 2),
              box.width = c(1,5,5,.1)/10,
              box.ratio = 2, stack = F,
              horizontal = FALSE,
              auto.key=list(space='right',title='released', reverse.rows = T))

tab__(Arrests, ~  year+colour + released) %>%
 barchart(ylab = 'count',
              ylim = c(0,1000),
              #  box.width = rep(.9^c(2,1,1,1,1,1), each = 2),
              box.width = c(1,5,5,.1)/10,
              box.ratio = 2, stack = F,
              horizontal = FALSE,
              auto.key=list(space='right',title='released', reverse.rows = T))

```
As we mentioned before, the interaction between colour and year is significant. So we explored the relationship between colour and the response variable in different years. As shown in the graph, the release rate among the white is always higher than among the black. Thus there always exists racial discrimination from 1997 to 2002. And the gap of release rate between the white and the black gets larger from 1997 to 2000, but it gets smaller from 2000 to 2002. Therefore we can see the discrimination of race is most serious in the year of 2000, and it is least severe in 2002. The situation of  discrimination differs in different years.

### The relationship between ‘employed’ and ‘released’ 
```{r,echo=FALSE}
tab__(Arrests, ~  employed+year+colour + released) %>%
     barchart(ylab = 'count',
              ylim = c(0,1000),
              #  box.width = rep(.9^c(2,1,1,1,1,1), each = 2),
              box.width = c(1,5,5,.1)/10,
              box.ratio = 2, stack = F,
              horizontal = FALSE,
              auto.key=list(space='right',title='released', reverse.rows = T))

```
The interaction term of employed and year is significant in our model, and since colour is a potential confounding factor, so we when we look at the influence of employed on release rate in different years , we need to control the ‘colour’ factor.  From the bar chart , we can see that when we control the colour to be white, the gap of the release rate among those who are employed are overall higher than those who are not employed. And the gap of the release rate becomes larger and larger from year 1997 to 2000, and becomes smaller from 2000 to 2002.  ANd for the black group, in the year of 1997, the trend is similar to the white group, only slightly different, the gap of the release rate is larger from 2017 to 2001, and the gap comes to a peak in year 2001, then the gap shrinks in the year 2002. In other words, the discrimination with respect to employment varies in different years. For the white people, discrimination is most severe in the year 2000, and least severe in 2002. And for the black people, discrimination is most severe in 2001, and least severe in 2002 if we look at the gap between the release rate among different groups.

# Conclusion

For model comparison part, we can see the best model also performs well in prediction evaluation. And in the added variable plot we can see that the interaction terms based on ‘colour’ have a strong influence on the dependent variable ‘released’. From the xy plot, we can conclude that being employed and being citizen could increase the probability of being released greatly for both white and black young people. As we see that ‘colour’ is a potential confounding factor, so we separate them into two groups, which are the white and the black. By looking at the frequency plots for colour-released we see that there exists racial discrimination from 1997 to 2002 since overall the release rate for white people is higher than the black. The situation of racial discrimination varies in different years, and the situation is the most serious in 2000. For the employed -released plot, it is clear to see that the release rate of people who are employed are higher than those who are not employed. Thus the discrimination with respect to employment always exists from 1997 to 2002, but also varies in different years when we control the factor ‘colour’.




